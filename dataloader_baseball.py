# -*- coding: utf-8 -*-
"""dataloader_baseball

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1VzQZJAUDije0VUmqKaW3ElpN7MPvnSHq
"""

import os
import xml.etree.ElementTree as ET
from collections import defaultdict
import cv2
import torch
from torch.utils.data import Dataset, DataLoader
import torchvision
from torchvision import transforms
import numpy as np
import matplotlib.pyplot as plt

# Print torch and torchvision versions for debugging
print(f"Torch version: {torch.__version__}")
print(f"Torchvision version: {torchvision.__version__}")


# -----------------------------
# 1) Parse CVAT XML annotations
# -----------------------------
def parse_cvat_xml(xml_path):
    tree = ET.parse(xml_path)
    root = tree.getroot()
    frames = defaultdict(list)

    for track in root.findall("track"):
        label = track.attrib["label"]
        for box in track.findall("box"):
            if int(box.attrib["outside"]) == 1:
                continue
            frame_id = int(box.attrib["frame"])
            xtl, ytl, xbr, ybr = map(
                float, [box.attrib["xtl"], box.attrib["ytl"], box.attrib["xbr"], box.attrib["ybr"]]
            )
            frames[frame_id].append({"label": label, "bbox": [xtl, ytl, xbr, ybr]})
    return frames


# --------------------------------------------
# 2) PyTorch Dataset for video + XML annotations
# --------------------------------------------
class CVATVideoDataset(Dataset):
    def __init__(self, xml_path, video_path, label_map=None, transforms=None):
        self.annotations = parse_cvat_xml(xml_path)
        self.cap = cv2.VideoCapture(video_path)
        self.total_frames = int(self.cap.get(cv2.CAP_PROP_FRAME_COUNT))
        self.transforms = transforms
        self.label_map = label_map or {"baseball": 1}

    def __len__(self):
        return self.total_frames

    def __getitem__(self, idx):
        self.cap.set(cv2.CAP_PROP_POS_FRAMES, idx)
        ret, frame = self.cap.read()
        if not ret:
            raise IndexError(f"Frame {idx} not found")
        frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)

        annots = self.annotations.get(idx, [])
        boxes = [obj["bbox"] for obj in annots]
        labels = [self.label_map.get(obj["label"], 0) for obj in annots]

        boxes = torch.as_tensor(boxes, dtype=torch.float32)
        labels = torch.as_tensor(labels, dtype=torch.int64)

        target = {"boxes": boxes, "labels": labels}

        if self.transforms:
            frame = self.transforms(frame)

        return frame, target


# -------------------------
# 3) Model setup
# -------------------------
def get_model(num_classes):
    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(weights="DEFAULT")
    in_feats = model.roi_heads.box_predictor.cls_score.in_features
    model.roi_heads.box_predictor = torchvision.models.detection.faster_rcnn.FastRCNNPredictor(
        in_feats, num_classes
    )
    return model


# -------------------------
# 4) Training loop
# -------------------------
def train_model(model, data_loader, device, epochs=5, lr=0.005):
    model.to(device)
    params = [p for p in model.parameters() if p.requires_grad]
    optimizer = torch.optim.SGD(params, lr=lr, momentum=0.9, weight_decay=0.0005)
    lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)

    for epoch in range(epochs):
        model.train()
        total_loss = 0
        for imgs, targets in data_loader:
            imgs = [img.to(device) for img in imgs]
            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]
            loss_dict = model(imgs, targets)
            losses = sum(loss for loss in loss_dict.values())

            optimizer.zero_grad()
            losses.backward()
            optimizer.step()
            total_loss += losses.item()

        lr_scheduler.step()
        print(f"Epoch [{epoch+1}/{epochs}] â€” Loss: {total_loss/len(data_loader):.4f}")

    print("âœ… Training complete!")
    return model


# -------------------------
# 5) Visualize predictions
# -------------------------
@torch.no_grad()
def visualize_predictions(model, dataset, device, num_samples=3):
    model.eval()
    for i in range(num_samples):
        img, _ = dataset[i]
        prediction = model([img.to(device)])[0]
        boxes = prediction["boxes"].cpu().numpy()
        scores = prediction["scores"].cpu().numpy()

        img_np = img.permute(1, 2, 0).cpu().numpy()
        for box, score in zip(boxes, scores):
            if score > 0.6:
                x1, y1, x2, y2 = box.astype(int)
                cv2.rectangle(img_np, (x1, y1), (x2, y2), (255, 0, 0), 2)
                cv2.putText(img_np, f"{score:.2f}", (x1, y1 - 5),
                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 0, 0), 2)
        plt.imshow(img_np)
        plt.title("Predictions")
        plt.axis("off")
        plt.show()


# -------------------------
# 6) Run training pipeline
# -------------------------
if __name__ == "__main__":
    XML_PATH = "IMG_8029_ann.xml"         # your XML file
    VIDEO_PATH = "IMG_8029_joel.mov"      # must match source in XML

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    dataset = CVATVideoDataset(XML_PATH, VIDEO_PATH, transforms=transforms.ToTensor())
    loader = DataLoader(dataset, batch_size=2, shuffle=True,
                        collate_fn=lambda x: tuple(zip(*x)))

    num_classes = 2  # background + baseball
    model = get_model(num_classes)
    model = train_model(model, loader, device, epochs=5, lr=0.005)

    # Save weights
    torch.save(model.state_dict(), "baseball_detector.pth")
    print("ðŸ’¾ Model saved to baseball_detector.pth")

    # Preview predictions
    visualize_predictions(model, dataset, device, num_samples=3)

import inspect
import torchvision.ops.boxes as box_ops

try:
    source_code = inspect.getsource(box_ops.clip_boxes_to_image)
    with open("clip_boxes_to_image_source.py", "w") as f:
        f.write(source_code)
    print("Source code of clip_boxes_to_image saved to clip_boxes_to_image_source.py")
except TypeError:
    print("Could not get source code for clip_boxes_to_image. It might be a built-in or compiled function.")